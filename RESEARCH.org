* devcontainer up hangs on Podman start event                :podman:devcontainer:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Intermittent hangs (~50/50) during =jolo create= / =devcontainer up= after the "Container started" log line.

** Root cause
The =@devcontainers/cli= uses =podman events --filter event=start= to track readiness. In rootless Podman environments, the default =events_logger= is often =journald=, which can silently drop events or have delivery delays. If the container starts before the listener is attached or the event is dropped, the CLI hangs indefinitely.

** Fix/Workaround
1. Switch Podman to file-based logging in =~/.config/containers/containers.conf=:
   #+begin_src toml
   [engine]
   events_logger = "file"
   #+end_src
2. Implement a timeout + =podman inspect= readiness check in the =jolo= wrapper as a fail-safe.

** Refs
- [[https://github.com/microsoft/vscode-remote-release/issues/7980][microsoft/vscode-remote-release#7980]]
- [[https://github.com/devcontainers/cli/issues/816][devcontainers/cli #816]]
- [[https://github.com/containers/podman/issues/21685][containers/podman#21685]]

* Gemini CLI crashes on Alpine/musl                        :musl:nodejs:gemini:
:PROPERTIES:
:DATE: 2026-02-09
:END:

node-pty prebuilt binary segfaults during PTY cleanup on musl libc.
The command itself executes fine — crash happens on PTY destroy/resize after exit.

** Root cause

~@lydell/node-pty-linux-x64/pty.node~ is compiled against glibc.
~ldd~ shows missing symbols: ~__asprintf_chk~, ~fcntl64~ (glibc-specific),
plus ~napi_*~ symbols (resolved by Node at runtime, not an issue).

~gcompat~ makes the binary loadable but ~forkpty()~ still segfaults
at the native level during cleanup — uncatchable by JS try/catch.

** Fix

Set ~tools.shell.enableInteractiveShell: false~ in ~settings.json~.
This bypasses node-pty entirely and uses ~child_process.spawn()~ fallback
(see ~shellExecutionService.js:86-99~).

Applied in ~_jolo/setup.py:setup_credential_cache()~ — merges the setting
into gemini's settings.json when copying credentials to container.

** Key files

- ~_jolo/setup.py~ — merges ~enableInteractiveShell: false~ after credential copy
- ~Containerfile~ — ~procps~ (GNU pgrep), ~diffutils~ (GNU diff for apheleia)
- Gemini source: ~shellExecutionService.js~, ~getPty.js~, ~shell-utils.js~

** Refs

- https://github.com/google-gemini/gemini-cli/issues/14087 (Alpine crash, open)
- https://github.com/google-gemini/gemini-cli/issues/4676 (Alpine shell hang)
- https://github.com/google-gemini/gemini-cli/issues/3448 (bash not found)

* jolo --sync now refreshes template skills               :jolo:skills:sync:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Added template skill syncing to all --sync flows so projects created earlier
can pick up new skills (e.g., research, feature-workflow) without re-creating.
Also handles dangling .claude/.gemini skills symlinks by checking for symlink
existence before creating new ones.

** Behavior
- `jolo up --sync`, `jolo tree --sync`, `jolo init --sync`, `jolo spawn --sync`
  call `sync_skill_templates()` to copy template skills into `.agents/skills`.
- Ensures `.claude/skills` and `.gemini/skills` symlinks exist.

** Key files
- `_jolo/setup.py` — `sync_skill_templates()` implementation
- `_jolo/commands.py` — hook in --sync paths

** What ~enableInteractiveShell: false~ loses

- Color output (stripped to plain text)
- Interactive programs (vim, less) don't work in shell tool
- ~isatty()~ returns false

None of this matters for AI agent usage in YOLO mode.

* BusyBox vs GNU tools in Alpine container                 :alpine:busybox:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Alpine ships BusyBox which lacks many GNU flags. Tools that break:

| Tool | Missing flag | Fix package |
|------+--------------+-------------|
| pgrep | ~-g~ (process group) | ~procps~ |
| diff | ~--rcs~, ~--strip-trailing-cr~, ~--text~ | ~diffutils~ |

Emacs apheleia uses ~diff --rcs~ for formatting patches — fails silently
with BusyBox, shows "Output file descriptor of apheleia-diff is closed".

* DONE OAuth token expiration in devcontainers          :oauth:claude:credentials:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Claude Code OAuth tokens expire multiple times per day in jolo devcontainers.

** Root cause

=setup_credential_cache()= copies =~/.claude/.credentials.json= to
=.devcontainer/.claude-cache/= at launch. The copy is bind-mounted RW into
the container. When Claude Code refreshes the token inside the container,
the new token writes to the copy — NOT back to host =~/.claude/=.
Next =jolo up= overwrites the refreshed token with stale host credentials.

Additionally, Claude Code has bugs where it doesn't actually use the refresh
token in several scenarios (copied creds, multi-instance, exit/restart).

** Fix (implemented)

Replaced the single =.claude-cache= directory mount with selective file mounts:
- =~/.claude/.credentials.json= — mounted RW from host (token refreshes persist)
- =~/.claude/statsig/= — mounted RO from host (feature flags)
- =settings.json= — still copied to =.claude-cache/= (notification hooks injected)
- =.claude.json= — still copied (MCP config injection)

This keeps =projects/=, =history.jsonl=, =todos/= invisible to the container
while letting token refreshes write back to the host.

** Key files
- =_jolo/constants.py= — =BASE_MOUNTS= (3 selective mounts replace 1 dir mount)
- =_jolo/setup.py= — =setup_credential_cache()= no longer copies credentials

** Open issues (anthropics/claude-code)

- [[https://github.com/anthropics/claude-code/issues/21765][#21765]] — refresh token not used on copied credentials (thumbs-upped, subscribed)
- [[https://github.com/anthropics/claude-code/issues/16957][#16957]] — exit-time refresh tokens not persisted
- [[https://github.com/anthropics/claude-code/issues/22600][#22600]] — multi-instance race condition
- [[https://github.com/anthropics/claude-code/issues/22602][#22602]] — VS Code reuses expired tokens across windows

* OpenSpec / Spec-Driven Development trial                 :sdd:openspec:workflow:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Tried OpenSpec v1.1.1 as a Spec-Driven Development framework for this project.
Full cycle on the credential mount change: new → ff → apply → verify → archive.

** What it is

OpenSpec creates structured markdown artifacts (proposal → design → specs → tasks)
in =openspec/= before implementation. Specs accumulate in =openspec/specs/= as a
living system description. Supports Claude, Gemini, Codex via slash commands.

Install: =pnpm install -g @fission-ai/openspec@latest=
Init: =openspec init --tools claude,codex,gemini=

** What we learned

The credential mount change required ~10 lines of code changes. OpenSpec generated
17KB / 359 lines of spec artifacts across 7 files. 100:1 ceremony-to-code ratio.

The verify step caught one real issue (stale docstring). A =git diff= review
would have caught the same thing.

** Verdict: not for this project

The existing stack (AGENTS.md + TODO.org + feature-workflow skill) covers 80% of
the value with zero overhead. The TODO.org entry for this feature already had
design tradeoffs, upstream issue links, and alternatives — which is most of what
OpenSpec's proposal + design artifacts provided, just in a different format.

** Where it might earn its keep

- 3+ parallel agents on the same feature needing a shared checklist
  (=tasks.md= survives context resets, which is genuinely useful)
- Projects where design rationale needs to survive beyond chat history
  and is non-obvious from the code
- Large teams where multiple people need to stay aligned on what to build

** Files

OpenSpec files are in =openspec/= on the =sdd= branch as a reference.

* Generated project docs layout                             :templates:structure:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Moved =TODO.org= and =RESEARCH.org= from project root to =docs/= for
generated projects. Keeps root clean for code files.

** Changes made

- =templates/TODO.org= -> =templates/docs/TODO.org=
- Created =templates/docs/RESEARCH.org=
- =templates/AGENTS.md= updated to reference =docs/= paths
- =_jolo/setup.py:copy_template_files()= now copies =docs/= as template dir
- Meta-project keeps its own TODO.org and RESEARCH.org at root

* jolo create slowness from Emacs cache copy             :emacs:perf:jolo:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Observed ~jolo create~ taking ~40s while ~devcontainer up~ is ~2s and
~podman run~ is ~0.5s.

** Root cause
Emacs cache (~/.cache/emacs) was being copied as part of Emacs config staging.
Cache size was ~4.2G. Copy dominates runtime even if config itself is small.

** Fix
Exclude cache-like directories during Emacs config copy (e.g. elpaca/straight/elpa,
eln-cache, tree-sitter, auto-save-list, tramp, server). Keep caches in
~/.cache/emacs-container~ bind mounts instead of copying into workspace.

** Evidence
~rsync~ of ~~/.config/emacs~ alone is ~0.05s, while cache size is ~4.2G.

** Key files
- ~_jolo/setup.py~ (setup_emacs_config ignore patterns)

* Eliminate per-project Dockerfile and devcontainer exec overhead :performance:jolo:podman:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Additional startup optimizations beyond the emacs cache fix.

** Per-project Dockerfile eliminated

Each project generated a Dockerfile (~FROM emacs-gui~ + ~apk add nodejs npm~ +
user label). The devcontainer CLI ran two builds (features layer + UID remap)
on every launch even when cached. Switched to ~"image"~ in devcontainer.json
with ~"updateRemoteUserUID": false~. ~DOCKERFILE_TEMPLATE~ constant removed.

** Direct podman exec instead of devcontainer CLI

Every ~devcontainer exec~ spawns Node.js, parses config, resolves features.
- ~is_container_running()~ now uses ~podman ps --filter~ directly
- ~_runtime_exec()~ helper tries ~podman exec~ first, falls back to devcontainer CLI
- Init commands batched into single exec with ~&&~

** GPG keyserver fetch removed from entrypoint

~gpg --keyserver keys.openpgp.org --recv-keys~ ran on every boot. Public keyring
already mounted read-only from host — fetch was redundant and caused variable
3s-2min delays depending on network.

** devcontainer CLI post-start delays (Arch Linux + podman)

Research found three CLI-side delays worth investigating if startup is still slow:
- ~updateRemoteUserUID~ CLI default may override JSON setting
  (use ~--update-remote-user-uid-default never~)
- ~userEnvProbe~ runs interactive login shell with 10s timeout
  (set ~"userEnvProbe": "none"~ in devcontainer.json)
- Podman ~events_logger=journald~ on Arch can stall 10-20s
  (set ~events_logger = "file"~ in ~containers.conf~)
- Ref: [[https://github.com/microsoft/vscode-remote-release/issues/7980][vscode-remote-release#7980]] (Arch + podman events hang)

** Rename

~run_default_mode~ renamed to ~run_up_mode~ — ~--help~ is the actual default,
~up~ is the subcommand.

* Agent session-end hooks (Claude / Gemini / Codex)      :agents:hooks:
:PROPERTIES:
:DATE: 2026-02-09
:END:

** Claude Code (confirmed)

Claude Code supports hooks configured in:
- =~/.claude/settings.json=
- =.claude/settings.json=
- =.claude/settings.local.json=

Hook events include =Stop=, =SessionStart=, and =SessionEnd=. The official
hooks reference documents the hook events, configuration structure, and that
=SessionEnd= runs when a session ends. citeturn1search0

** Gemini CLI (confirmed)

Gemini CLI documents lifecycle hooks including =SessionStart= and =SessionEnd=,
configured in =~/.gemini/settings.json= or =.gemini/settings.json=. The hooks
reference specifies event names and configuration structure. citeturn0search1turn0search8

** Codex CLI (no official hooks documented)

OpenAI’s official Codex CLI docs do not mention hooks or a notify configuration.
Treat any notification mechanism as unsupported until OpenAI documents it. citeturn0search6

* Zimfw race condition on container startup                :zsh:zimfw:perf:race:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Multiple tmux windows starting simultaneously caused sporadic ~git symbolic-ref~
errors from zimfw.

** Symptoms

#+begin_example
x modules/input: Error during git symbolic-ref. Use zmodule option -z|--frozen...
  fatal: ref refs/remotes/origin/HEAD is not a symbolic ref
#+end_example

Different number of modules fail each time. Started appearing after Emacs config
copy was optimized (faster startup = more concurrent shell inits).

** Root cause

Tmuxinator launches 5 windows (emacs, claude, gemini, codex, shell) simultaneously.
Each window starts zsh, which sources ~.zshrc~. The ~.zshrc~ zimfw bootstrap
(lines 121-138) checks if ~init.zsh~ is older than ~.zimrc~ and runs
~zimfw init -q~ if so. With 5 shells racing, multiple ~zimfw init~ processes
do concurrent git operations on the same module repos under ~~/.zim/modules/~.

The ~pre~ action in ~zimfw.zsh~ (line 877) runs ~git symbolic-ref --short
refs/remotes/origin/HEAD~ on each module. Concurrent git operations on the
same repos corrupt or conflict on the ref files.

** Fix

Bake zimfw into the container image. Added to Containerfile:

#+begin_src dockerfile
COPY --chown=$USERNAME:$USERNAME container/zimrc $HOME/.zimrc
RUN curl -fsSL -o $HOME/.zim/zimfw.zsh --create-dirs \
        https://github.com/zimfw/zimfw/releases/latest/download/zimfw.zsh && \
    zsh -c "ZIM_HOME=$HOME/.zim source $HOME/.zim/zimfw.zsh init -q"
#+end_src

At runtime, ~init.zsh~ is already built and newer than ~.zimrc~, so the
~.zshrc~ check skips init entirely. No git operations, no race.

** What didn't work

- ~on_project_start~ in tmuxinator dev.yml — command was ignored or too late
- ~tmux-layout.sh~ pre-init with ~if [ -f zimfw.zsh ]~ — skipped on first boot
  because zimfw wasn't downloaded yet (it's not in dotfile mounts)

** Key files

- ~Containerfile~ — zimfw download + init at build time
- ~container/zimrc~ — module list baked into image
- ~container/tmux-layout.sh~ — reverted workaround
- ~~/.zshrc~ lines 121-138 — bootstrap code (bind-mounted from host, readonly)
- ~~/.zim/zimfw.zsh~ line 877 — the ~git symbolic-ref~ that races

* wt worktree manager permission denied on /workspaces    :wt:container:permissions:
:PROPERTIES:
:DATE: 2026-02-09
:END:

~wt new foo~ fails: ~fatal: could not create leading directories of '/workspaces/foo/.git': Permission denied~

** Root cause

~/workspaces/~ is owned by ~root:root~ with ~drwxr-xr-x~. The devcontainer
runtime creates it when mounting the project, defaulting to root ownership.
~wt~ creates worktrees as sibling dirs under ~/workspaces/~, which requires
write access to the parent.

** Fix

Added to Containerfile (root-level RUN block, before ~USER $USERNAME~):
#+begin_src dockerfile
mkdir -p /workspaces && chown $USERNAME:$USERNAME /workspaces
#+end_src

The devcontainer mount overlays the project subdirectory but the parent dir
retains its image-layer permissions.

Also replaced ~just tree~ recipe (which only called ~wt new~) with a full
passthrough ~just wt~ recipe that supports all subcommands.

** Key files
- ~Containerfile~ — ownership fix
- ~container/wt~ — worktree manager script
- ~justfile~ — ~wt~ recipe

* ntfy.sh agent completion notifications                   :ntfy:notifications:hooks:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Push notifications when AI agents finish, via ntfy.sh HTTP API.

** Architecture
- ~container/notify-done~ — POSIX shell script, POSTs to ntfy.sh
- Hooks injected into agent settings at container launch time
- Claude: ~SessionEnd~ hook in ~.claude-cache/settings.json~
- Gemini: ~SessionEnd~ hook in ~.gemini-cache/settings.json~
- Codex: ~notify~ key in ~.codex-cache/config.toml~ (undocumented, best-effort)
- Env vars: ~NTFY_SERVER~ (from host), ~NTFY_TOPIC~ (defaults to project name), ~PROJECT~

** Key decisions
- ~SessionEnd~ not ~Stop~ for Claude — Stop fires on every response, spamming
  notifications in interactive sessions. Caught by Codex review.
- ~set -e~ + ~|| true~ on curl — script must never fail the agent hook
- ~--max-time 5~ on curl — prevents network hangs stalling agent shutdown
- ~[ ! -t 0 ]~ guard on stdin — prevents blocking when run manually
- ~_load_json_safe()~ — corrupt settings.json won't crash jolo up
- TOML check uses ~line.strip().startswith("notify")~ not substring match

** Self-hosted setup
User runs ntfy via docker compose on ~burial.ts.glvortex.net:9080~.
Host env: ~export NTFY_SERVER=http://burial.ts.glvortex.net:9080~
Phone app: ntfy Android via Obtainium from GitHub releases.

** Key files
- ~container/notify-done~ — notification script
- ~_jolo/setup.py~ — ~setup_notification_hooks()~ + ~_load_json_safe()~
- ~_jolo/container.py~ — NTFY env vars in containerEnv
- ~_jolo/commands.py~ — wired into all 5 launch flows
- ~docs/plan-notifications-claude.org~ — full design doc

** Branch Comparison Findings (notify-done vs feat/ntfy-notify-hooks) :musl:json:
The ~notify-done~ branch was chosen as the production path over ~feat/ntfy-notify-hooks~
due to several critical technical differences:

- **Hook Structure**: Claude and Gemini require a nested JSON structure for hooks:
  ~hooks > EventName > [ { hooks: [ { type: "command", command: "..." } ] } ]~.
  The ~feat~ branch used a flat array which is incompatible with current agent versions.
- **Script Robustness**: The ~notify-done~ script in the current branch correctly drains
  ~stdin~. Agents often pipe a JSON context (containing session IDs, etc.) to hooks.
  If the script doesn't read this, it can cause SIGPIPE or block the agent's exit.
- **Codex Support**: Both branches implemented Codex support by appending to
  ~.codex-cache/config.toml~, but the ~notify-done~ implementation uses the same
  environment variable pattern as the other agents for consistency.
- **File Permissions**: The ~feat~ branch accidentally changed the notification
  script to mode 644 (non-executable). Corrected to 755 in this branch.

* Devcontainer "Container started" sleep loop is not root hang :devcontainer:podman:debug:
:PROPERTIES:
:DATE: 2026-02-09
:END:

When ~devcontainer up~ starts a container, it injects a keep-alive command:

#+begin_src
/bin/sh -c 'echo Container started; trap "exit 0" 15; exec "$@"; while sleep 1 & wait $!; do :; done -'
#+end_src

This shows up in ~podman top~ as a ~sleep 1~ loop, but it is *normal* and
indicates the devcontainer CLI is keeping the container alive for exec/hooks.
It is not itself the cause of a hang.

** Evidence
- ~devcontainer up --log-level trace~ completed with ~"outcome":"success"~
  and ~LifecycleCommandExecutionMap~ empty (no hooks running).
- ~podman inspect <container> --format '{{.Config.Cmd}}'~ shows the keep-alive loop.

** Where the real hang likely is
If ~jolo create~ appears to hang after "Container started", the stall is
likely in the *next step* (e.g. a ~devcontainer exec~ that starts tmux,
tmuxinator, or shell init). Run the suspected exec with ~--log-level trace~
to surface the failing command.

** Useful commands
- ~devcontainer exec --log-level trace --workspace-folder <host-path> -- /bin/sh -lc '...command...'~
- ~podman exec -it <ctr> sh -lc 'ps -ef'~
- ~podman logs <ctr>~

* Eglot Python server prompt due to multiple servers        :emacs:eglot:python:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Eglot prompts for Python server selection when multiple servers are available
(e.g. ~pyright-langserver~ and ~ruff~). This prevents auto-start on file open.

** Config locations
- Eglot config: =~/.config/emacs/modules/bergheim-eglot.el~
- Apheleia config (global mode): =~/.config/emacs/modules/formating.el~

** Current state
- Apheleia is enabled globally but only maps JS/TS/CSS/JSON to ~biome~.
- No Python formatter is configured in Apheleia.

** Fix
Pin Eglot to a single Python LSP (e.g. ~pyright-langserver~) to avoid prompts.
If using Apheleia for Python formatting (e.g. ~ruff format~), add a Python
formatter mapping in ~formating.el~ to avoid conflicts with LSP formatting.

* gh auth token not available in devcontainers             :gh:credentials:jolo:
:PROPERTIES:
:DATE: 2026-02-09
:END:

~gh auth token~ fails inside jolo devcontainers even though ~/.config/gh~ is
mounted read-only from host.

** Root cause

Newer ~gh~ versions store OAuth tokens in the OS keyring (e.g. ~gnome-keyring~,
~kwallet~, macOS Keychain) instead of writing them to ~hosts.yml~. The mounted
~hosts.yml~ contains user/protocol info but no ~oauth_token~ field. The container
has no access to the host keyring.

** Fix

Pass ~GH_TOKEN~ as a ~containerEnv~ variable, populated by ~get_secrets()~ which
tries (in order):
1. ~GH_TOKEN~ env var
2. ~GITHUB_TOKEN~ env var
3. ~gh auth token~ CLI output (reads from host keyring)

The ~${localEnv:GH_TOKEN}~ in devcontainer.json picks up the value set by
~os.environ.update(secrets)~ in the jolo launch flow.

** Key files
- ~_jolo/container.py~ — added ~GH_TOKEN~ to ~containerEnv~
- ~_jolo/setup.py~ — ~get_secrets()~ fetches token via ~gh auth token~
- ~.devcontainer/devcontainer.json~ — added ~GH_TOKEN~ entry

** Note
Existing containers need ~jolo up --sync --new~ to pick up the change.
The ~gh~ config mount (~~/.config/gh~ readonly) is kept for non-token config.

* Feature workflow reviews alternate agents               :process:skills:
:PROPERTIES:
:DATE: 2026-02-09
:END:

Updated the feature-workflow skill to require Review 2 to use a different
external agent than Review 1 to ensure fresh eyes on correctness.

** Key files
- .agents/skills/feature-workflow/SKILL.md

* Skills deduplication via symlink chain                :skills:architecture:
:PROPERTIES:
:DATE: 2026-02-11
:END:

Eliminated duplication between =.agents/skills/= and =templates/.agents/skills/=.
Skills are now authored once in =templates/.agents/skills/= and the rest is symlinks.

** Symlink chain
#+begin_example
templates/.agents/skills/<name>/SKILL.md   # single source of truth
.agents/skills -> ../templates/.agents/skills  # project root symlink
.claude/skills -> ../.agents/skills            # Claude discovery
.gemini/skills -> ../.agents/skills            # Gemini discovery
#+end_example

** Decision: AGENTS.md stays separate
=AGENTS.md= (meta-project) and =templates/AGENTS.md= (generated projects) have
genuinely different content. No symlink — each maintained independently.

* Ruff vs Apheleia formatting fight                     :ruff:emacs:pre-commit:
:PROPERTIES:
:DATE: 2026-02-11
:END:

Emacs save-on-format and pre-commit hooks disagreed on Python formatting,
causing every commit to fail with ruff-format changes.

** Root causes (three layered problems)

1. *Pre-commit used different ruff version*: pinned remote repo had ruff 0.8.6,
   system had 0.14.9. Fixed by switching to =repo: local= with =language: system=.

2. *Apheleia defaulted to black, not ruff*: =apheleia-mode-alist= maps
   =python-mode= to =black= by default. Black was installed in Alpine.
   Fixed by adding =(setf (alist-get 'python-mode apheleia-mode-alist) 'ruff)=
   to =~/.config/emacs/modules/formating.el=.

3. *Unstaged pyproject.toml caused stash revert*: =pyproject.toml= was changed
   from =line-length = 120= to =line-length = 79= but not staged. Pre-commit
   stashes unstaged changes (=git stash --keep-index=) before running hooks,
   reverting pyproject.toml to the committed 120 value. Emacs ruff read 79
   from the working tree; pre-commit ruff read 120 from HEAD.

** Fix

- =.pre-commit-config.yaml=: =repo: local=, =language: system= for ruff
- =formating.el=: explicit ruff mapping for python-mode and python-ts-mode
- Staged pyproject.toml + all reformatted files together in one commit

** Key insight

=apheleia-formatters-respect-fill-column= defaults to nil. Apheleia does NOT
pass =--line-length= to ruff. Both Emacs and pre-commit read line-length
from =pyproject.toml= — the only question is /which version/ of pyproject.toml
they see (working tree vs stashed HEAD).

** Key files
- =.pre-commit-config.yaml= — local system ruff hooks
- =pyproject.toml= — =line-length = 79=
- =~/.config/emacs/modules/formating.el= — ruff overrides for Python

* sync_skill_templates self-destructs via symlink          :skills:jolo:symlink:
:PROPERTIES:
:DATE: 2026-02-11
:END:

~sync_skill_templates~ in ~_jolo/setup.py~ crashed with ~FileNotFoundError~
when run on the meta-project (emacs-container itself).

** Root cause

In this repo, ~.agents/skills~ is a symlink to ~../templates/.agents/skills~.
The sync function's destination (~target_dir/.agents/skills~) resolves through
the symlink to the same path as the source (~templates/.agents/skills~). The
loop calls ~shutil.rmtree(dst)~ before ~shutil.copytree(entry, dst)~ — rmtree
destroys the source through the symlink, then copytree fails on the deleted dir.

** Fix

Resolve both paths and skip sync when they're identical (commit ~8317bc6~).

** Related

Commit ~1ee5d3a~ ("Remove local skills") also dropped ~afk~ and ~db-reset~
skills. They existed only in ~.agents/skills/~ (local) and were never copied
to ~templates/.agents/skills/~ before the local copies were deleted. Restored
in the same fix commit.

* TERM/term-keys mismatch in devcontainer                 :tmux:terminfo:emacs:
:PROPERTIES:
:DATE: 2026-02-11
:END:

User observed that Emacs + term-keys works over SSH but not inside devcontainer.
Recent local changes forced TERM=tmux-direct in the container and devcontainer env.
If tmux-direct terminfo is missing in the container, term-keys and Emacs key
translations can break.

** Current local diff
- Containerfile: added tmux-direct to terminal-features and set TERM=tmux-direct
- _jolo/container.py: containerEnv TERM set to tmux-direct

** Next checks
- Ensure /usr/share/terminfo/t/tmux-direct exists in container image
- If missing, either install terminfo or fallback to tmux-256color
- Verify tmux default-terminal and terminal-overrides for key passthrough

* Pre-commit hook caching requires PRE_COMMIT_HOME in container  :precommit:hooks:container:
:PROPERTIES:
:DATE: 2026-02-11
:END:

Warm-cache in the image alone did not prevent first-commit hook downloads.
Root cause: pre-commit uses =~/.cache/pre-commit= by default unless
~PRE_COMMIT_HOME~ is set inside the running container.

** Fix
- Bake hook environments into the image.
- Set ~PRE_COMMIT_HOME=/opt/pre-commit-cache~ in the devcontainer env
  so runtime uses the baked cache.

** Key files
- ~Containerfile~ (pre-commit install-hooks + cache path)
- ~_jolo/container.py~ (devcontainer.json env)

* DONE Claude CLI uses API key over Max subscription           :claude:auth:
:PROPERTIES:
:DATE: 2026-02-11
:END:

~claude -p~ returned "Credit balance is too low" because ~ANTHROPIC_API_KEY~
in the environment takes precedence over Max subscription OAuth.

** Credential precedence (hardcoded, no override)

1. ~ANTHROPIC_API_KEY~ (env var) — highest, always wins
2. ~CLAUDE_CODE_OAUTH_TOKEN~ (env var)
3. ~~/.claude/.credentials.json~ (stored OAuth from ~/login~)

No ~--use-subscription~ flag exists. ~forceLoginMethod: "claudeai"~ in
settings.json only affects the login screen, not runtime auth precedence.
Multiple GitHub issues requesting this (#12861, #9880, #8327) — all closed
with no resolution as of Feb 2026.

** Fix

Strip ~ANTHROPIC_API_KEY~ from the claude command via shell alias in
~Containerfile~. The key stays in the container env for Emacs and other tools.

#+begin_src dockerfile
alias claude="env -u ANTHROPIC_API_KEY claude --dangerously-skip-permissions"
#+end_src

** Refs
- [[https://github.com/anthropics/claude-code/issues/12861][#12861]] — Option to prefer subscription auth (closed/stale)
- [[https://github.com/anthropics/claude-code/issues/9880][#9880]] — Add flag to ignore API key (closed as duplicate)
- [[https://github.com/anthropics/claude-code/issues/8327][#8327]] — API key overrides Max/Pro subscription
- [[https://github.com/anthropics/claude-code/issues/16489][#16489]] — forceLoginMethod bug (open)

* Pre-commit hook warmup needs git repo in Containerfile      :precommit:container:hooks:
:PROPERTIES:
:DATE: 2026-02-11
:END:

~pre-commit install-hooks~ fails outside a git repo. Warmup in the image must
create a temporary git repo before installing hooks.

** Fix
- Run hook warmup in a temp git repo (pre-commit requires one):

#+begin_src dockerfile
RUN cd /tmp && git init pre-commit-repo && cd pre-commit-repo && \
    pre-commit install-hooks -c /tmp/pre-commit-hooks.yaml && \
    cd / && rm -rf /tmp/pre-commit-repo /tmp/pre-commit-hooks.yaml
#+end_src

- Remove biome/markdownlint hooks from warmup list — pre-commit's
  ~language: node~ hardcodes ~npm~ internally, and Alpine has pnpm not npm.
  ~language_version: system~ avoids the node download but still calls npm.
- Install ~markdownlint-cli~ via pnpm global instead.
- Projects use ~repo: local~ with ~language: system~ for node-based hooks.

** Key files
- ~Containerfile~ (hook warmup, pnpm globals, pre-commit-hooks.yaml)

* Stash shared mount for devcontainers                 :jolo:devcontainer:mounts:
:PROPERTIES:
:DATE: 2026-02-11
:END:

Added a default RW shared mount intended as a non-reproducible "stash"
for ad-hoc notes and scratch files.

** Decision
- Host path: ~/stash
- Container path: /workspaces/stash
- Default-on, explicitly labeled as non-reproducible in MOTD

** Implementation
- Added mount to ~BASE_MOUNTS~ (devcontainer.json generation)
- Ensure host directory exists before container start
- MOTD banner includes stash warning

** Key files
- ~_jolo/constants.py~ (BASE_MOUNTS)
- ~_jolo/setup.py~ (setup_stash)
- ~_jolo/commands.py~ (call setup_stash)
- ~container/motd~ (stash warning)

* Stash default path + init flow guard               :jolo:devcontainer:init:
:PROPERTIES:
:DATE: 2026-02-11
:END:

Renamed the shared mount from "junk drawer" to "stash" and ensured the
host path exists before devcontainer startup, including ~jolo init~.

** Decision
- Host path: ~/stash
- Container path: /workspaces/stash
- Always create host path via ~setup_stash()~ before ~devcontainer_up~

** Key files
- ~_jolo/constants.py~ (BASE_MOUNTS)
- ~_jolo/setup.py~ (setup_stash)
- ~_jolo/commands.py~ (call setup_stash in init/up/tree/create/spawn)
- ~container/motd~ (stash label)
- ~templates/AGENTS.md~ (stash guidance)
