#+TITLE: Devcontainer Start Race Condition (podman events)
#+DATE: 2026-02-09

* Context

We observed intermittent hangs (~50/50) during =jolo create= / =devcontainer up= after the log line:

- "Container started"

The hang occurs even when nothing changes between runs and can be reproduced back-to-back. A manual =sleep 1= between delete/create appears to reduce incidence, but does not eliminate it.

- **Update (2026-02-09)**: User reported a hang even with =jolo delete --purge --yes ./p3 && sleep 1; jolo create p3 -v --lang rust=. The CLI hung indefinitely after "Container started".

* Findings

1. The container *is* running when the hang occurs.
   - =podman ps= shows the container up and healthy.
   - =podman logs= shows the start message only.

2. The devcontainer CLI appears blocked waiting for a start event.
   - The process list shows it sitting on:
     - =podman events --format json --filter event=start=
   - This behavior is in the upstream devcontainer CLI, not our own app code.

3. Restarting the container immediately unblocks the CLI.
   - =podman restart p2= emits a new start event.
   - The waiting =podman events= command returns instantly.

4. This strongly indicates a race:
   - The container starts before the events listener is attached.
   - The start event is missed, so the CLI waits forever.

5. Version note:
   - Observed with =@devcontainers/cli= 0.80.0 (from verbose startup log).

* Plan For Fix

We need a workaround in *jolo* (wrapper-side), unless we plan to patch or upstream a fix to the devcontainer CLI.

1. Make readiness detection race-free in jolo.
   - Prefer polling =podman inspect= for =State.Running=true= with a short timeout.
   - This avoids relying on event streams entirely.

2. Optional mitigation (not fully race-free):
   - Use =podman events --since <timestamp>= to reduce (not eliminate) the race window.

3. Keep success behavior unchanged, but fail fast with a clear error if readiness never arrives.

4. Validate the fix by running =jolo create= repeatedly *without* sleeps and confirming no hangs.

* Suggestions (Codex)

- **Upstream fix (preferred):** patch =@devcontainers/cli= to remove the race by attaching the event listener *before* the container start call, or by seeding the events stream with =--since= anchored just before start. This is proper event handling, not a workaround.
- **Wrapper-level guard (safe fallback):** if =podman inspect= already reports the container as running, skip waiting for a start event. This avoids the hang without adding sleeps or poll loops, and preserves normal behavior when the event stream works.

* Suggestions (Gemini)

- **Fix the Entrypoint Contract**: The =devcontainer= CLI often runs a probe command (e.g., =echo Container started=) to verify readiness. Our current =entrypoint.sh= ignores all arguments (=$@=) and ends in =exec sleep infinity=. This "swallows" the probe command, causing the CLI to hang waiting for a return that never comes.
- **The Fix**:
  1. Update =entrypoint.sh= to =exec "$@"= if arguments are provided.
  2. Simplify the =Containerfile= ENTRYPOINT to call the script directly (remove the =sh -c= wrapper).
- **Benefit**: This makes the container "well-behaved" according to the =devcontainer= spec and likely removes the state-mismatch causing the hang.

* Analysis (Claude)

** Gemini's entrypoint suggestion does not apply

The =echo Container started= probe runs via =podman exec= into the already-running container, not as the container's CMD. The ENTRYPOINT (=sh -c "exec $HOME/entrypoint.sh $@"=) correctly keeps the container alive with =exec sleep infinity=. The probe succeeds — "Container started" is printed. The hang occurs /before/ the probe, while the CLI waits for the =podman events= start event that was already emitted.

Changing the entrypoint to =exec "$@"= would make the container exit after the probe (since =echo= returns immediately), which is the opposite of what we want.

** Codex's analysis is correct

The race is definitively: =podman events --filter event=start= listener attaches /after/ =podman start= completes. The event fires before anyone is listening. The CLI waits forever.

The =sleep 1= workaround does not help because it only delays the /next/ create — it does not change the ordering between the container start and the event listener within a single =devcontainer up= invocation.

** What devcontainer up actually does post-start (in our case: nothing)

With our config (=userEnvProbe: "none"=, no lifecycle commands, no features, =updateRemoteUserUID: false=), the CLI has nothing to do after confirming the container is running. It just returns the JSON result. This means if the container IS running, the =devcontainer up= process is safe to kill — no post-start work is lost.

** Recommended fix: timeout with container-running check

Rather than hacking around the race (background restarts, sleeps), use a timeout on =devcontainer up= and verify the container independently:

1. Run =devcontainer up= with a 15-second timeout.
2. If it completes normally, done.
3. If it times out, check =podman inspect= for =State.Running == true=.
4. If running, kill the hung CLI process and return success.
5. If not running, return failure.

This is not a hack — the container genuinely is ready. We are just not waiting for a buggy event listener to acknowledge something that already happened. The workaround is confined to a single function (=devcontainer_up()=) and does not change the happy path.

* Open Questions

- Is this Podman-specific, or reproducible with Docker? (Docker may buffer events differently.)
- Should we open an upstream issue/PR against =@devcontainers/cli= after confirming?

* Notes

- The issue is not app code or container runtime; it is event-handling in the startup orchestration.
- The sleep workaround likely reduces the window where the event is missed.
